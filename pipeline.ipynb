{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7e06-2eac-4a2b-9cc5-2749bba378bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end pipeline for YouTube comment crawling, preprocessing, topic modeling (BERTopic), and sentiment/emotion analysis.\n",
    "!pip install youtube-comment-downloader bertopic transformers nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8e2732-666d-4601-bff3-dbbb5e82698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 128 comments to 'guardian.json' (JSON Lines format)\n"
     ]
    }
   ],
   "source": [
    "# --- Crawl YouTube Comments ---\n",
    "from itertools import islice\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader, SORT_BY_POPULAR\n",
    "import json\n",
    "\n",
    "def crawl_comments(url: str, output_file: str = \"ENDEVR.json\", limit: int = None):\n",
    "    \"\"\"Download YouTube comments and save them in JSON Lines format.\"\"\"\n",
    "    downloader = YoutubeCommentDownloader()\n",
    "    comments = downloader.get_comments_from_url(url, sort_by=SORT_BY_POPULAR)\n",
    "    \n",
    "    if limit:\n",
    "        comments = islice(comments, limit)\n",
    "    \n",
    "    comments_list = list(comments)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for comment in comments_list:\n",
    "            json_line = json.dumps(comment, ensure_ascii=False)\n",
    "            f.write(json_line + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Saved {len(comments_list)} comments to '{output_file}' (JSON Lines format)\")\n",
    "\n",
    "# URL = \"https://www.youtube.com/watch?v=LTduwK0-sGI\"\n",
    "URL = \"https://www.youtube.com/watch?v=5udOx8-QxtE\"\n",
    "crawl_comments(URL, output_file=\"GUARDIAN.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b61cd8-fc06-43db-9872-89d52667f655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed 128 comments and saved to 'GUARDIAN_clean.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>channel</th>\n",
       "      <th>votes</th>\n",
       "      <th>replies</th>\n",
       "      <th>photo</th>\n",
       "      <th>heart</th>\n",
       "      <th>reply</th>\n",
       "      <th>time_parsed</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugw2APWQ4_TqgbM4Xlh4AaABAg</td>\n",
       "      <td>this is guaranteed to keep people stuck in the...</td>\n",
       "      <td>vor 1 Jahr</td>\n",
       "      <td>@ArjunaJackson</td>\n",
       "      <td>UCKQs8X3QxiRWlS8XbXTYTFA</td>\n",
       "      <td>258</td>\n",
       "      <td>6</td>\n",
       "      <td>https://yt3.ggpht.com/usI7nJIo6lj0prOOD036PVnz...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.731407e+09</td>\n",
       "      <td>this is guaranteed to keep people stuck in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgzlVq6zVImVxnyQ5ZN4AaABAg</td>\n",
       "      <td>No, it could make the grieving process worse a...</td>\n",
       "      <td>vor 1 Jahr</td>\n",
       "      <td>@skylark1237</td>\n",
       "      <td>UCANRu8T_HKO96XcA4glmqZA</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_n8LDP5zMqUT9S1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.731407e+09</td>\n",
       "      <td>no it could make the grieving process worse an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgyinvHQnO3yMyw8bdB4AaABAg</td>\n",
       "      <td>We are living in a black mirror episode.</td>\n",
       "      <td>vor 9 Monaten</td>\n",
       "      <td>@58rocKsErt1</td>\n",
       "      <td>UCHp8UCCFeHCG5t_mOWRStGQ</td>\n",
       "      <td>32</td>\n",
       "      <td></td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_nWb1yghKJ8-Sce...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.739356e+09</td>\n",
       "      <td>we are living in a black mirror episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgwoImAwQT4ljgcNgl94AaABAg</td>\n",
       "      <td>This may not be healthy</td>\n",
       "      <td>vor 1 Jahr</td>\n",
       "      <td>@averageamericangirl6819</td>\n",
       "      <td>UC2aTKfLcNgjmQXKkDws10Uw</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_law9lQDSwoio5B...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.731407e+09</td>\n",
       "      <td>this may not be healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgwnN6IZI0DsWrR9zs54AaABAg</td>\n",
       "      <td>If AI ended grief, that wouldn't speak well of...</td>\n",
       "      <td>vor 1 Jahr</td>\n",
       "      <td>@jps0117</td>\n",
       "      <td>UC4xTJlctOrJqqce3Cfthl_g</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_n-095R21h9D80v...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.731407e+09</td>\n",
       "      <td>if ai ended grief that wouldnt speak well of w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cid  \\\n",
       "0  Ugw2APWQ4_TqgbM4Xlh4AaABAg   \n",
       "1  UgzlVq6zVImVxnyQ5ZN4AaABAg   \n",
       "2  UgyinvHQnO3yMyw8bdB4AaABAg   \n",
       "3  UgwoImAwQT4ljgcNgl94AaABAg   \n",
       "4  UgwnN6IZI0DsWrR9zs54AaABAg   \n",
       "\n",
       "                                                text           time  \\\n",
       "0  this is guaranteed to keep people stuck in the...     vor 1 Jahr   \n",
       "1  No, it could make the grieving process worse a...     vor 1 Jahr   \n",
       "2           We are living in a black mirror episode.  vor 9 Monaten   \n",
       "3                            This may not be healthy     vor 1 Jahr   \n",
       "4  If AI ended grief, that wouldn't speak well of...     vor 1 Jahr   \n",
       "\n",
       "                     author                   channel votes replies  \\\n",
       "0            @ArjunaJackson  UCKQs8X3QxiRWlS8XbXTYTFA   258       6   \n",
       "1              @skylark1237  UCANRu8T_HKO96XcA4glmqZA   188       2   \n",
       "2              @58rocKsErt1  UCHp8UCCFeHCG5t_mOWRStGQ    32           \n",
       "3  @averageamericangirl6819  UC2aTKfLcNgjmQXKkDws10Uw    90       1   \n",
       "4                  @jps0117  UC4xTJlctOrJqqce3Cfthl_g    71       2   \n",
       "\n",
       "                                               photo  heart  reply  \\\n",
       "0  https://yt3.ggpht.com/usI7nJIo6lj0prOOD036PVnz...  False  False   \n",
       "1  https://yt3.ggpht.com/ytc/AIdro_n8LDP5zMqUT9S1...  False  False   \n",
       "2  https://yt3.ggpht.com/ytc/AIdro_nWb1yghKJ8-Sce...  False  False   \n",
       "3  https://yt3.ggpht.com/ytc/AIdro_law9lQDSwoio5B...  False  False   \n",
       "4  https://yt3.ggpht.com/ytc/AIdro_n-095R21h9D80v...  False  False   \n",
       "\n",
       "    time_parsed                                              clean  \n",
       "0  1.731407e+09  this is guaranteed to keep people stuck in the...  \n",
       "1  1.731407e+09  no it could make the grieving process worse an...  \n",
       "2  1.739356e+09            we are living in a black mirror episode  \n",
       "3  1.731407e+09                            this may not be healthy  \n",
       "4  1.731407e+09  if ai ended grief that wouldnt speak well of w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Clean and Preprocess Comments ---\n",
    "import pandas as pd\n",
    "import re, string\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"Lowercase, remove links/punctuation, normalize whitespace.\"\"\"\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"http\\S+|www\\S+\", \"\", txt)\n",
    "    txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "def preprocess(input_file: str = \"GUARDIAN.json\", output_file: str = \"GUARDIAN_clean.csv\"):\n",
    "    \"\"\"Load, clean, and save preprocessed comments.\"\"\"\n",
    "    data = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Preprocessed {len(df)} comments and saved to '{output_file}'\")\n",
    "    return df\n",
    "\n",
    "df = preprocess(\"GUARDIAN.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6e5b58-a053-4cd0-81e2-e2141e390c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 11:39:06,443 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.44s/it]\n",
      "2025-11-12 11:39:14,474 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-12 11:39:14,476 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-12 11:39:18,847 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-12 11:39:18,849 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-12 11:39:18,864 - BERTopic - Cluster - Completed âœ“\n",
      "2025-11-12 11:39:18,871 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-12 11:39:18,897 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Topics:\n",
      "\n",
      "   Topic  Count                        Name  \\\n",
      "0     -1      8    -1_loads_934_crew_actual   \n",
      "1      0     76     0_grief_would_ai_people   \n",
      "2      1     30  1_back_exactly_yeah_thanks   \n",
      "3      2     14  2_black_mirror_episode_one   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [loads, 934, crew, actual, recreation, red, dw...   \n",
      "1  [grief, would, ai, people, think, like, let, l...   \n",
      "2  [back, exactly, yeah, thanks, right, well, awf...   \n",
      "3  [black, mirror, episode, one, ended, current, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [charlie brooker is an actual visionary, if yo...  \n",
      "1  [spot on finally someone gets that its humans ...  \n",
      "2  [be right back, richardduplessis1090 for the b...  \n",
      "3  [black mirror, its black mirror, this is a bla...  \n",
      "âœ… BERTopic results saved to 'GUARDIAN_btopic.csv'\n",
      "\n",
      "--- Topic 0: [('grief', np.float64(0.05572500279519491)), ('would', np.float64(0.041150210854549264)), ('ai', np.float64(0.041150210854549264)), ('people', np.float64(0.03696036992942539)), ('think', np.float64(0.03247582942209174)), ('like', np.float64(0.03188146919812383)), ('let', np.float64(0.029291720870271314)), ('love', np.float64(0.025922363353962342)), ('someone', np.float64(0.025922363353962342)), ('us', np.float64(0.024157492063696908))] ---\n",
      "â€¢ this is guaranteed to keep people stuck in the past and not able to let go and move on. ...\n",
      "\n",
      "â€¢ No, it could make the grieving process worse and leave people unable to deal with loss or accept reality. ...\n",
      "\n",
      "â€¢ If AI ended grief, that wouldn't speak well of what grief must be: an irrevocable loss. ...\n",
      "\n",
      "â€¢ \"I wanna be able to continue having conversations with my mother\"\n",
      "\n",
      "Yeah I'm sure you do, but whether or not we should be able to do that is another question.\n",
      "\n",
      "Maybe grief is an important part of the human experience. After all, what is grief, but love persevering? ...\n",
      "\n",
      "â€¢ Instead of ending grief we should learn how to live with it ...\n",
      "\n",
      "â€¢ I didnt had the chance to talk to my father before the died, we hadnt spoken in a few years. Something I regreat. Not sure how I fell about it, talking to an AI, but I dont think It would help. Sometimes, I visit his resting place, talk a bit, clean it and leave. I think it helps ...\n",
      "\n",
      "\n",
      "--- Topic 1: [('back', np.float64(0.16090846574981282)), ('exactly', np.float64(0.16090846574981282)), ('yeah', np.float64(0.16090846574981282)), ('thanks', np.float64(0.16090846574981282)), ('right', np.float64(0.1445765151497935)), ('well', np.float64(0.1387680742199058)), ('awful', np.float64(0.09810030500607171)), ('545', np.float64(0.09810030500607171)), ('highlight', np.float64(0.09810030500607171)), ('cheers', np.float64(0.09810030500607171))] ---\n",
      "â€¢ This may not be healthy ...\n",
      "\n",
      "â€¢ Good thought ...\n",
      "\n",
      "â€¢ ðŸŽ¯ ...\n",
      "\n",
      "â€¢ I agree ...\n",
      "\n",
      "â€¢ Exactly. Its as awful as mediumship. 5:45 hope you didn't pay her for that bollards. ...\n",
      "\n",
      "â€¢ Exactly.  Thank you. ...\n",
      "\n",
      "\n",
      "--- Topic 2: [('black', np.float64(0.43845848095609885)), ('mirror', np.float64(0.4003173717624456)), ('episode', np.float64(0.24116669304240795)), ('one', np.float64(0.13631916251164727)), ('ended', np.float64(0.12790160098062045)), ('current', np.float64(0.07797716551764675)), ('copawany7171', np.float64(0.07797716551764675)), ('birthday', np.float64(0.07797716551764675)), ('attic', np.float64(0.07797716551764675)), ('holograms', np.float64(0.07797716551764675))] ---\n",
      "â€¢ We are living in a black mirror episode. ...\n",
      "\n",
      "â€¢ Black mirror already did this. Can't remember how the episode ended. ...\n",
      "\n",
      "â€¢ Btw,I watched one minute before OFF! ...\n",
      "\n",
      "â€¢ Hint: it didn't end well ...\n",
      "\n",
      "â€¢ What is black mirror? ...\n",
      "\n",
      "â€¢ Spoilers: It ended by her trying to get the robot to shut itself off via one big drop on a rock in the ocean, and then not being able to really live with her decision to do that, and then a cut to x-years later. So instead she locked it in the attic and only lets her daughter visit it on her birthday. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling with BERTopic ---\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def run_bertopic(df, output_file: str = \"GUARDIAN_btopic.csv\"):\n",
    "    \"\"\"Run BERTopic and save topic assignments.\"\"\"\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer,\n",
    "        language=\"english\",\n",
    "        verbose=True,\n",
    "        calculate_probabilities=False\n",
    "    )\n",
    "\n",
    "    topics, _ = topic_model.fit_transform(df[\"clean\"])\n",
    "    df[\"topic\"] = topics\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(\"\\nTop 10 Topics:\\n\")\n",
    "    print(topic_info.head(10))\n",
    "\n",
    "    print(f\"âœ… BERTopic results saved to '{output_file}'\")\n",
    "    return topic_model, topic_info, df\n",
    "\n",
    "topic_model, topic_info, df = run_bertopic(df)\n",
    "\n",
    "for i in topic_info.head(11)[\"Topic\"]:\n",
    "    if i == -1:  # -1 means outliers\n",
    "        continue\n",
    "    print(f\"\\n--- Topic {i}: {topic_model.get_topic(i)} ---\")\n",
    "    sample_comments = df[df[\"topic\"] == i][\"text\"].head(6).tolist()\n",
    "    for c in sample_comments:\n",
    "        print(\"â€¢\", c[:500], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a24f89-b5e3-40da-85b8-f5d97e1451b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sentiment predictions saved to 'GUARDIAN_sent.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion predictions saved to 'GUARDIAN_emo.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Sentiment and Emotion Analysis ---\n",
    "from transformers import pipeline\n",
    "\n",
    "# Models: you can change any of these with models from huggingface\n",
    "MODEL_SENT = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "MODEL_EMO = \"cardiffnlp/twitter-roberta-large-emotion-latest\"\n",
    "\n",
    "# Label maps\n",
    "id2label_sent = { \"0\": \"Negative\", \"1\": \"Neutral\", \"2\": \"Positive\" }\n",
    "id2label_emo = {\n",
    "    \"0\": \"anger\", \"1\": \"anticipation\", \"2\": \"disgust\", \"3\": \"fear\",\n",
    "    \"4\": \"joy\", \"5\": \"love\", \"6\": \"optimism\", \"7\": \"pessimism\",\n",
    "    \"8\": \"sadness\", \"9\": \"surprise\", \"10\": \"trust\"\n",
    "}\n",
    "\n",
    "def get_prediction(text, pipe, label_map):\n",
    "    \"\"\"Run model prediction and map label IDs to names.\"\"\"\n",
    "    try:\n",
    "        result = pipe(text[:256])[0]\n",
    "        label_id = result[\"label\"].replace(\"LABEL_\", \"\")\n",
    "        label_name = label_map.get(label_id, result[\"label\"])\n",
    "        return label_name, float(result[\"score\"])\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def analyze_sentiment(df, output_sent=\"GUARDIAN_sent.csv\", output_emo=\"GUARDIAN_emo.csv\"):\n",
    "    \"\"\"Perform sentiment and emotion classification.\"\"\"\n",
    "    # Sentiment\n",
    "    sent_pipe = pipeline(\"sentiment-analysis\", model=MODEL_SENT)\n",
    "    df[[\"sent_label\", \"sent_score\"]] = df[\"clean\"].apply(\n",
    "        lambda x: pd.Series(get_prediction(x, sent_pipe, id2label_sent))\n",
    "    )\n",
    "    df.to_csv(output_sent, index=False)\n",
    "    print(f\"âœ… Sentiment predictions saved to '{output_sent}'\")\n",
    "\n",
    "    # Emotion\n",
    "    emo_pipe = pipeline(\"sentiment-analysis\", model=MODEL_EMO)\n",
    "    df[[\"emo_label\", \"emo_score\"]] = df[\"clean\"].apply(\n",
    "        lambda x: pd.Series(get_prediction(x, emo_pipe, id2label_emo))\n",
    "    )\n",
    "    df.to_csv(output_emo, index=False)\n",
    "    print(f\"âœ… Emotion predictions saved to '{output_emo}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = analyze_sentiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c99ce-0d15-44e7-a61e-4d912b5a135c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
