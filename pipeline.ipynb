from itertools import islice
from youtube_comment_downloader import *

import json, re, string
import pandas as pd
import nltk
from nltk.corpus import stopwords

from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from transformers import pipeline

## crawling
downloader = YoutubeCommentDownloader()
comments = downloader.get_comments_from_url('https://www.youtube.com/watch?v=LTduwK0-sGI', sort_by=SORT_BY_POPULAR)

comments_list = list(comments)  # change 100 to how many you want, or remove islice for all

output_file = "ENDEVR.json"
with open(output_file, "w", encoding="utf-8") as f:
    for comment in comments_list:
        json_line = json.dumps(comment, ensure_ascii=False)
        f.write(json_line + "\n")

print(f"Saved {len(comments_list)} comments to '{output_file}'")

## preprocessing
input_data = "ENDEVR.json"
out_berttopic = "ENDEVR_btopic"
out_sent = "ENDEVR_sent"
out_emo = "ENDEVR_emo"

data = []
with open(input_data, "r") as f:
    for line in f:
        line = line.strip()
        if line:
            try:
                data.append(json.loads(line))
            except:
                pass

df = pd.DataFrame(data)
print(f"Loaded {len(df)} comments")
df = df.dropna(subset=["text"])
df["text"] = df["text"].astype(str)

def clean_text(txt):
    txt = txt.lower()
    txt = re.sub(r"http\S+|www\S+", "", txt)
    txt = txt.translate(str.maketrans('', '', string.punctuation))
    txt = re.sub(r"\s+", " ", txt)
    return txt.strip()

df["clean"] = df["text"].apply(clean_text)
print (df["clean"][5]

## bertopic 
stop_words = stopwords.words("english")
vectorizer = CountVectorizer(stop_words=stop_words)
topic_model = BERTopic(vectorizer_model=vectorizer,
                       language="english",
                       verbose=True,
                       calculate_probabilities=False)
topics, probs = topic_model.fit_transform(df["clean"])

# topic_model = BERTopic(language="english", verbose=True, calculate_probabilities=False)
# topics, probs = topic_model.fit_transform(df["clean"])

df["topic"] = topics
topic_info = topic_model.get_topic_info()

print("\nTop 10 Topics:\n")
print(topic_info.head(10))

df.to_csv(out_berttopic+"_2.csv", index=False)
print("Done! BERTopic results saved")

for i in topic_info.head(11)["Topic"]:
    if i == -1:  # -1 means outliers
        continue
    print(f"\n--- Topic {i}: {topic_model.get_topic(i)} ---")
    sample_comments = df[df["topic"] == i]["text"].head(6).tolist()
    for c in sample_comments:
        print("•", c[:500], "...\n")

## sentiment analysis
model_emo_rlarge = "cardiffnlp/twitter-roberta-large-emotion-latest"
model_emo_rbase = "cardiffnlp/twitter-roberta-base-emotion-latest"
model_sent_rbase = "cardiffnlp/twitter-roberta-base-sentiment-latest"
model_emo_dbertbase ="bhadresh-savani/distilbert-base-uncased-emotion"

# Load sentiment analysis pipeline
sentiment_model = pipeline(
    "sentiment-analysis",
    model=model_emo_rlarge
)

id2label = { 
    "0": "anger", "1": "anticipation", "2": "disgust", "3": "fear",
    "4": "joy", "5": "love", "6": "optimism", "7": "pessimism",
    "8": "sadness", "9": "surprise", "10": "trust" 
}

id2label_sent = { 
    "0": "Negative", "1": "Neutral", "2": "Positive"
}

def get_emotion_hf(text):
    try:
        result = sentiment_model(text[:256])[0]  # truncate to 256 tokens
        label_id = result["label"].replace("LABEL_", "")  # extract numeric ID
        label_name = id2label.get(label_id, result["label"])  # map to emotion
        return label_name, float(result["score"])
    except:
        return None, None

def get_sentiment_hf(text):
    try:
        result = sentiment_model(text[:256])[0]   # truncate to 512 tokens
        label_id = result["label"].replace("LABEL_", "")  # extract numeric ID
        label_name = id2label_sent.get(label_id, result["label"])  # map to emotion
        return label_name, float(result["score"])
        # return result["label"], float(result["score"])
    except:
        return None, None

# df[["hf_label", "hf_score"]] = df["clean"].apply(lambda x: pd.Series(get_sentiment_hf(x)))
# print(df["hf_label"].value_counts())
# df.to_csv(out_sent+".csv", index=False)
# print(f"✅ Predictions saved as {out_sent}.csv")

df[["hf_label", "hf_score"]] = df["clean"].apply(lambda x: pd.Series(get_emotion_hf(x)))
print(df["hf_label"].value_counts())
df.to_csv(out_emo+".csv", index=False)
print(f"✅ Predictions saved as {out_sent}.csv")
